<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> CLIP FUNDAMENTALS | Giordano Cicchetti </title> <meta name="author" content="Giordano Cicchetti "> <meta name="description" content="Simple introduction to the CLIP (Contrastive Language-Image Pretraining) world."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://giordano-cicchetti.github.io/blog/2025/CLIP-Fundamentals/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightbox2@2.11.5/dist/css/lightbox.min.css" integrity="sha256-uypRbsAiJcFInM/ndyI/JHpzNe6DtUNXaWEUWEPfMGo=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@5.4.4/dist/photoswipe.min.css" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/spotlight.js@0.7.8/dist/css/spotlight.min.css" integrity="sha256-Dsvkx8BU8ntk9Iv+4sCkgHRynYSQQFP6gJfBN5STFLY=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/venobox@2.1.8/dist/venobox.min.css" integrity="sha256-ohJEB0/WsBOdBD+gQO/MGfyJSbTUI8OOLbQGdkxD6Cg=" crossorigin="anonymous"> </head> <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZWSV9V4RVV"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-ZWSV9V4RVV");</script> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Giordano Cicchetti</span> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">CLIP FUNDAMENTALS</h1> <p class="post-meta"> Created in June 12, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/contrastive-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Contrastive-Learning</a>   ·   <a href="/blog/category/sample-posts"> <i class="fa-solid fa-tag fa-sm"></i> sample-posts</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h3 id="topic-clip-contrastive-language-image-pretraining">Topic: CLIP: Contrastive Language-Image Pretraining</h3> <h3 id="author-giordano-cicchetti">Author: <a href="https://giordano-cicchetti.github.io/" rel="external nofollow noopener" target="_blank">Giordano Cicchetti</a> </h3> <p>This lab shows how to implement one of the most influential frameworks of recent years in the field of multimodal learning: <a href="https://arxiv.org/abs/2103.00020" rel="external nofollow noopener" target="_blank">CLIP (Radford et al., 2021)</a>.</p> <p>Here is a summary of what this lab will cover:</p> <ol> <li>General theoretical introduction</li> <li>How to implement the CLIP loss function</li> <li>How to pretrain image and text encoders using the CLIP loss function</li> <li>How to evaluate the knowledge acquired by these models</li> </ol> <p><strong>Suggested readings before starting</strong>:</p> <ul> <li><a href="https://arxiv.org/abs/2103.00020" rel="external nofollow noopener" target="_blank">CLIP Paper (Radford et al., 2021)</a></li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">open_clip_torch</span> <span class="o">--</span><span class="n">quiet</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="n">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span><span class="p">,</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="n">open_clip</span>
<span class="kn">from</span> <span class="n">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CocoCaptions</span>
<span class="kn">from</span> <span class="n">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
</code></pre></div></div> <h2 id="section-1-theoretical-introduction">Section 1: Theoretical introduction</h2> <p>CLIP (Contrastive Language–Image Pretraining) is a framework introduced by OpenAI that learns visual representations from natural language supervision. It achieves strong zero-shot performance on a wide range of image classification tasks.</p> <h3 id="key-ideas">Key Ideas</h3> <ul> <li>Learn a <strong>shared embedding space for text and images</strong>. Where these two modalities are connected to each other by means of semantics meaning (i.e. the embedding of the image of a dog is 'close' to the embedding of the text caption "the photo of a dog").</li> <li>Use <strong>contrastive loss</strong> to align matching image-text pairs and separate non-matching ones.</li> <li>Concepts of closeness and alignement expressed in terms of cosine <strong>similarity</strong>.</li> <li>Enables <strong>zero-shot classification</strong> without task-specific fine-tuning.</li> </ul> <h3 id="clip-architecture">CLIP Architecture</h3> <p>CLIP is composed of two encoders:</p> <ul> <li> <strong>Image Encoder</strong>: Typically ResNet-50 or ViT.</li> <li> <strong>Text Encoder</strong>: Typically Transformer-based.</li> </ul> <p>Both encoders project their inputs into a <strong>shared embedding space</strong>. The latent dimension of such latent space is an hyperparameter that can be choosed at design phase.</p> <h3 id="clip-use-case">CLIP Use Case</h3> <p>Main use: CLIP is used to pre-train text and visual models that can be used/incorporated in downstram tasks.</p> <p>Major examples are:</p> <ul> <li> <p>CLIP text encoder used as text conditioning module in text-to-content generation models [1-2];</p> </li> <li> <p>CLIP image encoder used in Multimodal LLM to encode visual informations. [3-4]</p> </li> </ul> <p>[1] Ramesh, Aditya, et al. "Hierarchical text-conditional image generation with clip latents." arXiv preprint arXiv:2204.06125 1.2 (2022): 3.</p> <p>[2] Liu, Haohe, et al. "Audioldm: Text-to-audio generation with latent diffusion models." International conference on machine learning. PMLR, 2023.</p> <p>[3] Liu, Haotian, et al. "Visual instruction tuning." Advances in neural information processing systems 36 NeurIPS (2023)</p> <p>[4] Li, Junnan, et al. "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models." International conference on machine learning. PMLR, 2023.</p> <h3 id="diagram">Diagram</h3> <p><img src="assets/img/CLIP_blog/CLIP_first_image.png" alt="Figure description"></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> % include figure.liquid loading="eager" path="assets/img/CLIP_blog/CLIP_first_image.png" class="img-fluid rounded z-depth-1" % </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/CLIP_blog/CLIP_first_image-480.webp 480w,/assets/img/CLIP_blog/CLIP_first_image-800.webp 800w,/assets/img/CLIP_blog/CLIP_first_image-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/CLIP_blog/CLIP_first_image.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="section-2-clip-loss-function">Section 2: CLIP Loss Function</h2> <p>CLIP uses a symmetric cross-entropy loss based on cosine similarities. This loss encourages matching image-text pairs to have similar embeddings while pushing apart non-matching pairs.</p> <p>Given a batch of N (image, text) pairs:</p> <ol> <li>Compute all pairwise similarities $S_{ij} = cosine(img_i, txt_j)$</li> <li>Scale by a learnable temperature parameter τ</li> <li>Compute cross-entropy loss both ways (image-to-text and text-to-image)</li> </ol> <p>Mathematically:</p> \[\mathcal{L}_{\mathrm{CLIP}} = -\frac{1}{2B} \sum_{i=1}^{B} \Biggl[ \underbrace{ \log \frac{ \exp\bigl(\langle \mathbf{I}_i, \mathbf{T}_i \rangle / \tau\bigr) }{ \sum_{j=1}^{B} \exp\bigl(\langle \mathbf{I}_i, \mathbf{T}_j \rangle / \tau\bigr) } }_{\text{I2T: Image-to-Text}} \;+\; \underbrace{ \log \frac{ \exp\bigl(\langle \mathbf{T}_i, \mathbf{I}_i \rangle / \tau\bigr) }{ \sum_{j=1}^{B} \exp\bigl(\langle \mathbf{T}_i, \mathbf{I}_j \rangle / \tau\bigr) } }_{\text{T2I: Text-to-Image}} \Biggr]\] <p>Let’s break it down into parts and analyze each component to better understand its behavior;</p> <h3 id="1-compute-all-pairwise-similarities">1) Compute all pairwise similarities</h3> \[\begin{equation} S_{ij} = cosine(img_i, txt_j) \end{equation}\] <p>The cosine similarity between two generic vectors $A$ and $B$ in the same dimensional space $\mathbb{R}^d$, is defined as the cosine value of the angle $\theta$ between them:</p> \[\begin{equation} cos(\theta)= \frac{\langle \mathbf{A}, \mathbf{B} \rangle}{||\mathbf{A}|| ||\mathbf{B}||} \end{equation}\] <p>In the clip loss function the cosine similarity is present in the dot products \(\begin{equation} \langle \textbf{I}_i, \textbf{T}_i \rangle \end{equation}\)</p> <p>The denominator?</p> <ul> <li>$\mathbf{I}_i$ and $\mathbf{T}_i$ are extracted embeddings from the image and text encoder.</li> <li>Key assumption of CLIP loss function is that $\mathbf{I}_i$ and $\mathbf{T}_i$ are vectors normalized to unitary norm. (i.e. we ALWAYS have to normalize embeddings coming out from encoders, unless explicitly defined)</li> <li>The same subscript $_i$ tell us that they corresponds to paired content with the same semantic.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">512</span><span class="p">)</span> <span class="c1"># Assume this is the output of the image encoder
</span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">512</span><span class="p">)</span> <span class="c1"># Assume this is the ouput of the text encoder
</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">The shape of i vector is </span><span class="si">{</span><span class="n">i</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">The shape of t vector is </span><span class="si">{</span><span class="n">t</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The shape of i vector is torch.Size([1, 512]) 
The shape of t vector is torch.Size([1, 512])
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Lets Check the norm of i and t
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">The norm of i is </span><span class="si">{</span><span class="n">i</span><span class="p">.</span><span class="nf">norm</span><span class="p">()</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">The norm of t is </span><span class="si">{</span><span class="n">t</span><span class="p">.</span><span class="nf">norm</span><span class="p">()</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The norm of i is 22.876724243164062
The norm of t is 22.4530086517334
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Normalize to unitary norm
</span><span class="n">i</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">normalize</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">normalize</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">The norm of i is </span><span class="si">{</span><span class="n">i</span><span class="p">.</span><span class="nf">norm</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">The norm of t is </span><span class="si">{</span><span class="n">t</span><span class="p">.</span><span class="nf">norm</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The norm of i is 1.00
The norm of t is 1.00
</code></pre></div></div> <p>Now that we have unitary norm vectors, we can compute the cosine similarity among them just with the dot product</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cosine_similarity</span> <span class="o">=</span> <span class="n">i</span> <span class="o">@</span> <span class="n">t</span><span class="p">.</span><span class="n">T</span> <span class="c1"># [1,512] @ [512,1] = [1,1]
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">The cosine similarity is </span><span class="si">{</span><span class="n">cosine_similarity</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The cosine similarity is -0.04
</code></pre></div></div> <p>Working with batches</p> <p>Assume you have a batch of N image-text pairs:</p> <p>Images → encoded as vectors $\mathbf{I}_1, \mathbf{I}_2, \cdots, \mathbf{I}_n$</p> <p>Texts → encoded as vectors $\mathbf{T}_1, \mathbf{T}_2, \cdots, \mathbf{T}_n$</p> <p>CLIP computes cosine similarities among all image-text combinations in the batch: \(\begin{equation} \mathbf{S}_{ ij} = ⟨\mathbf{I}_i,\mathbf{T}_j⟩ \end{equation}\)</p> <p>$\mathbf{S}$ is a matrix of shape [B, B], containing similarity scores.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">B</span> <span class="o">=</span> <span class="mi">5</span>

<span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">B</span><span class="p">,</span><span class="mi">512</span><span class="p">)</span> <span class="c1"># Assume this is the output of the image encoder
</span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">B</span><span class="p">,</span><span class="mi">512</span><span class="p">)</span> <span class="c1"># Assume this is the ouput of the text encoder
</span>
<span class="c1"># Normalize to unitary norm
</span><span class="n">i</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">normalize</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">normalize</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">S</span> <span class="o">=</span> <span class="n">i</span> <span class="o">@</span> <span class="n">t</span><span class="p">.</span><span class="n">T</span>   <span class="c1">#[B,512] @ [512,B] = [B,B]
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">The shape of S is </span><span class="si">{</span><span class="n">S</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">The shape of S is: </span><span class="se">\n</span><span class="si">{</span><span class="n">S</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The shape of S is torch.Size([5, 5])
The shape of S is: 
tensor([[ 0.0347,  0.0375, -0.0328, -0.0253, -0.0745],
        [ 0.0179,  0.0866, -0.0741, -0.0032, -0.0220],
        [-0.0151,  0.0109, -0.0295,  0.0496,  0.0703],
        [-0.0272, -0.0400, -0.0184,  0.0262,  0.0691],
        [ 0.0432, -0.0579,  0.0218, -0.0007, -0.0053]])
</code></pre></div></div> <h3 id="2-scale-by-a-learnable-temperature-parameter-τ">2) Scale by a learnable temperature parameter τ</h3> \[\begin{equation} \mathbf{S}_{ ij} = \frac{⟨\mathbf{I}_i,\mathbf{T}_j⟩}{\tau} \end{equation}\] <p>Effects of Temperature on Model Behavior</p> <ol> <li>Lower Temperature ($\tau$ ↓)</li> </ol> <ul> <li> <p><strong>Sharper softmax distribution</strong>: Emphasizes differences between positive and negative pairs.</p> </li> <li> <p><strong>Focus on hard negatives</strong>: The model pays more attention to challenging negative samples that are similar to the positive ones.</p> </li> <li> <p><strong>Risk of over-separation</strong>: May push apart semantically similar samples that shouldn't be separated, potentially harming generalization.</p> </li> </ul> <ol> <li>Higher Temperature ($\tau$ ↑)</li> </ol> <ul> <li> <p><strong>Smoother softmax distribution</strong>: Reduces emphasis on individual pair differences.</p> </li> <li> <p><strong>Tolerance to similar negatives</strong>: Allows semantically similar samples to remain closer in the embedding space.</p> </li> <li> <p><strong>Risk of under-separation</strong>: May not sufficiently distinguish between dissimilar samples, leading to less discriminative embeddings.</p> </li> </ul> <p>Practical Considerations</p> <ul> <li> <p><strong>Learnable τ</strong>: CLIP often treats τ as a learnable parameter, allowing the model to find an optimal value during training.</p> </li> <li> <p><strong>Dataset dependency</strong>: Optimal τ may vary based on dataset characteristics, such as class balance and semantic diversity.</p> </li> <li> <p><strong>Dynamic τ</strong>: Some approaches adjust τ during training to balance the focus between hard negatives and overall embedding structure.</p> </li> <li> <p>Typical temperature values range from 0.1 to 0.01. Temperature in the original CLIP paper: Learnable starting from 0.07</p> </li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">temperature</span> <span class="o">=</span> <span class="mf">0.07</span> <span class="c1">#Define the temperature parameter
</span>
<span class="n">S_scaled</span> <span class="o">=</span> <span class="n">S</span> <span class="o">/</span> <span class="n">temperature</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">The shape of S_scaled is </span><span class="si">{</span><span class="n">S_scaled</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">The shape of S_scaled is: </span><span class="se">\n</span><span class="si">{</span><span class="n">S_scaled</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The shape of S_scaled is torch.Size([5, 5])
The shape of S_scaled is: 
tensor([[ 0.4956,  0.5359, -0.4687, -0.3611, -1.0650],
        [ 0.2563,  1.2378, -1.0579, -0.0460, -0.3147],
        [-0.2156,  0.1557, -0.4212,  0.7079,  1.0038],
        [-0.3892, -0.5716, -0.2630,  0.3744,  0.9868],
        [ 0.6172, -0.8265,  0.3112, -0.0093, -0.0752]])
</code></pre></div></div> <h3 id="3-compute-cross-entropy-loss-both-ways-image-to-text-and-text-to-image">3) Compute cross-entropy loss both ways (image-to-text and text-to-image)</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Targets: index of matching pairs
</span><span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">B</span><span class="p">)</span> <span class="c1"># [0,1,2,3....,B]
</span>
<span class="c1"># Cross-entropy losses
</span><span class="n">loss_i2t</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">S_scaled</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>        <span class="c1"># Image to text
</span><span class="n">loss_t2i</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">S_scaled</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>      <span class="c1"># Text to image
</span>
<span class="c1"># Final CLIP loss
</span><span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss_i2t</span> <span class="o">+</span> <span class="n">loss_t2i</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">The final CLIP loss is </span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="assets/img/CLIP_blog/CLIP_second_image.png" alt="Figure description"></p> <h3 id="code">Code</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ClipLoss</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.07</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="c1"># Let the temperature be a learnable parameter
</span>        <span class="n">self</span><span class="p">.</span><span class="n">temperature</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">temperature</span><span class="p">))</span>
        <span class="c1"># Otherwise
</span>        <span class="c1"># self.temperature = torch.tensor(temperature)
</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">image_features</span><span class="p">,</span> <span class="n">text_features</span><span class="p">):</span>
        <span class="c1"># image features: [B,D]
</span>        <span class="c1"># text features: [B,D]
</span>
        <span class="c1"># Normalize
</span>        <span class="n">image_features</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">normalize</span><span class="p">(</span><span class="n">image_features</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">text_features</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">normalize</span><span class="p">(</span><span class="n">text_features</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Similarity matrix and temperature scaling
</span>        <span class="n">logits_per_image</span> <span class="o">=</span> <span class="n">image_features</span> <span class="o">@</span> <span class="n">text_features</span><span class="p">.</span><span class="nf">t</span><span class="p">()</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">temperature</span>
        <span class="n">logits_per_text</span> <span class="o">=</span> <span class="n">text_features</span> <span class="o">@</span> <span class="n">image_features</span><span class="p">.</span><span class="nf">t</span><span class="p">()</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">temperature</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">image_features</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">image_features</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>

        <span class="n">loss_i2t</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">logits_per_image</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">loss_t2i</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">logits_per_text</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

        <span class="nf">return </span><span class="p">(</span><span class="n">loss_i2t</span> <span class="o">+</span> <span class="n">loss_t2i</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
</code></pre></div></div> <h2 id="section-3-training-session">Section 3: Training session</h2> <h3 id="load-mini-coco-dataset">Load Mini-Coco Dataset</h3> <p>Handmade dataset from MSCOCO <a href="https://cocodataset.org/#home" rel="external nofollow noopener" target="_blank">https://cocodataset.org/#home</a></p> <p>1000 Image-Text pairs for training 100 Image-Text pairs for testing</p> <p>Just for fun :)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">gdown</span> <span class="mi">1</span><span class="n">ld44JrobUwxlSnLEncdXIqX2tCeZA</span><span class="o">-</span><span class="n">fd</span>

<span class="err">!</span><span class="n">unzip</span> <span class="o">/</span><span class="n">content</span><span class="o">/</span><span class="n">mini_coco</span><span class="p">.</span><span class="nb">zip</span>
</code></pre></div></div> <p>Retrieval tasks:</p> <ul> <li> <p>Image-to-Text Retrieval (Image → Text):</p> <p>Given an image as a query, the goal is to retrieve the most relevant textual descriptions (captions) from a collection. This is useful in applications like automatic image captioning or image search engines that return text results.</p> </li> <li> <p>Text-to-Image Retrieval (Text → Image):</p> <p>Given a text query (e.g., a sentence or phrase), the goal is to retrieve the most relevant images from a dataset. This is common in systems like search engines or content recommendation platforms where users describe what they want to see.</p> </li> </ul> <p><img src="assets/img/CLIP_blog/CLIP_Third_image.png" alt="Figure description"></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_retrieval_metrics</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>

    <span class="n">sx</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sort</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">diagonal</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">)</span>


    <span class="n">d</span> <span class="o">=</span> <span class="n">d</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Make it (N, 1)
</span>    <span class="n">ind_matrix</span> <span class="o">=</span> <span class="n">sx</span> <span class="o">-</span> <span class="n">d</span>
    <span class="n">matches</span> <span class="o">=</span> <span class="p">(</span><span class="n">ind_matrix</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">ind</span> <span class="o">=</span> <span class="n">matches</span><span class="p">.</span><span class="nf">float</span><span class="p">().</span><span class="nf">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">'</span><span class="s">R1</span><span class="sh">'</span><span class="p">:</span> <span class="nf">float</span><span class="p">((</span><span class="n">ind</span> <span class="o">==</span> <span class="mi">0</span><span class="p">).</span><span class="nf">sum</span><span class="p">().</span><span class="nf">item</span><span class="p">())</span> <span class="o">*</span> <span class="mi">100</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">ind</span><span class="p">),</span>
        <span class="sh">'</span><span class="s">R5</span><span class="sh">'</span><span class="p">:</span> <span class="nf">float</span><span class="p">((</span><span class="n">ind</span> <span class="o">&lt;</span> <span class="mi">5</span><span class="p">).</span><span class="nf">sum</span><span class="p">().</span><span class="nf">item</span><span class="p">())</span> <span class="o">*</span> <span class="mi">100</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">ind</span><span class="p">),</span>
        <span class="sh">'</span><span class="s">R10</span><span class="sh">'</span><span class="p">:</span> <span class="nf">float</span><span class="p">((</span><span class="n">ind</span> <span class="o">&lt;</span> <span class="mi">10</span><span class="p">).</span><span class="nf">sum</span><span class="p">().</span><span class="nf">item</span><span class="p">())</span> <span class="o">*</span> <span class="mi">100</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">ind</span><span class="p">),</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">metrics</span>
</code></pre></div></div> <p>Training and Validation pipeline</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># --- CONFIG ---
</span><span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">NUM_EPOCHS</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">DEVICE</span> <span class="o">=</span> <span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span>
<span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="sh">"</span><span class="s">RN50</span><span class="sh">"</span>
<span class="n">PRETRAINED</span> <span class="o">=</span> <span class="bp">None</span>  <span class="c1"># Start from scratch
</span>
<span class="c1"># --- DATASET ---
</span><span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">Resize</span><span class="p">((</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)),</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">(),</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">Normalize</span><span class="p">((</span><span class="mf">0.48145466</span><span class="p">,</span> <span class="mf">0.4578275</span><span class="p">,</span> <span class="mf">0.40821073</span><span class="p">),</span>
                         <span class="p">(</span><span class="mf">0.26862954</span><span class="p">,</span> <span class="mf">0.26130258</span><span class="p">,</span> <span class="mf">0.27577711</span><span class="p">)),</span>
<span class="p">])</span>


<span class="n">train_dataset</span> <span class="o">=</span> <span class="nc">CocoCaptions</span><span class="p">(</span>
    <span class="n">root</span><span class="o">=</span><span class="sh">'</span><span class="s">./mini_coco/train2017</span><span class="sh">'</span><span class="p">,</span>
    <span class="n">annFile</span><span class="o">=</span><span class="sh">'</span><span class="s">./mini_coco/annotations/captions_train2017.json</span><span class="sh">'</span><span class="p">,</span>
    <span class="n">transform</span><span class="o">=</span><span class="n">transform</span>
<span class="p">)</span>

<span class="n">val_dataset</span> <span class="o">=</span>  <span class="nc">CocoCaptions</span><span class="p">(</span>
    <span class="n">root</span><span class="o">=</span><span class="sh">'</span><span class="s">./mini_coco/val2017</span><span class="sh">'</span><span class="p">,</span>
    <span class="n">annFile</span><span class="o">=</span><span class="sh">'</span><span class="s">./mini_coco/annotations/captions_val2017.json</span><span class="sh">'</span><span class="p">,</span>
    <span class="n">transform</span><span class="o">=</span><span class="n">transform</span>
<span class="p">)</span>
<span class="k">def</span> <span class="nf">collate_fn</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
    <span class="n">images</span><span class="p">,</span> <span class="n">captions</span> <span class="o">=</span> <span class="nf">zip</span><span class="p">(</span><span class="o">*</span><span class="n">batch</span><span class="p">)</span>
    <span class="n">images</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
    <span class="n">texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">cap</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">cap</span> <span class="ow">in</span> <span class="n">captions</span><span class="p">]</span>  <span class="c1"># use the first caption
</span>    <span class="k">return</span> <span class="n">images</span><span class="p">,</span> <span class="n">texts</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                        <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_fn</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="n">val_loader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                        <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_fn</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># --- MODEL ---
</span><span class="n">model</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">preprocess</span> <span class="o">=</span> <span class="n">open_clip</span><span class="p">.</span><span class="nf">create_model_and_transforms</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="n">PRETRAINED</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">open_clip</span><span class="p">.</span><span class="nf">get_tokenizer</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="nc">ClipLoss</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.07</span><span class="p">)</span>

<span class="c1"># --- TRAINING ---
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">()]</span><span class="o">+</span><span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">loss_fn</span><span class="p">.</span><span class="nf">parameters</span><span class="p">()],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>

<span class="c1"># --- TRACK FOR PLOT ---
</span><span class="n">loss_values</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">temperature_values</span> <span class="o">=</span> <span class="p">[]</span>



<span class="c1"># --- TRAIN + VALIDATION ---
</span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">NUM_EPOCHS</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">texts</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s"> [Training]</span><span class="sh">"</span><span class="p">):</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
        <span class="n">texts</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="n">texts</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>

        <span class="n">image_features</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">encode_image</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">text_features</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">encode_text</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">image_features</span><span class="p">,</span> <span class="n">text_features</span><span class="p">)</span>

        <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>
        <span class="n">loss_values</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
        <span class="n">temperature_values</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">.</span><span class="n">temperature</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>

    <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s"> — Training Loss: </span><span class="si">{</span><span class="n">avg_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># --- VALIDATION ---
</span>    <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">all_image_features</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">all_text_features</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">texts</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">val_loader</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s"> [Validation]</span><span class="sh">"</span><span class="p">):</span>
            <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
            <span class="n">texts</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="n">texts</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>

            <span class="n">image_feats</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">normalize</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">encode_image</span><span class="p">(</span><span class="n">images</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">text_feats</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">normalize</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">encode_text</span><span class="p">(</span><span class="n">texts</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">all_image_features</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">image_feats</span><span class="p">)</span>
            <span class="n">all_text_features</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">text_feats</span><span class="p">)</span>

        <span class="n">image_features</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">(</span><span class="n">all_image_features</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">text_features</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">(</span><span class="n">all_text_features</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">similarity</span> <span class="o">=</span> <span class="n">image_features</span> <span class="o">@</span> <span class="n">text_features</span><span class="p">.</span><span class="n">T</span>

        <span class="n">metrics_i2t</span> <span class="o">=</span> <span class="nf">compute_retrieval_metrics</span><span class="p">(</span><span class="n">similarity</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Validation Accuracy — Image→Text: R1 </span><span class="si">{</span><span class="n">metrics_i2t</span><span class="p">[</span><span class="sh">'</span><span class="s">R1</span><span class="sh">'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">, R5 </span><span class="si">{</span><span class="n">metrics_i2t</span><span class="p">[</span><span class="sh">'</span><span class="s">R5</span><span class="sh">'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">, R10 </span><span class="si">{</span><span class="n">metrics_i2t</span><span class="p">[</span><span class="sh">'</span><span class="s">R10</span><span class="sh">'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

        <span class="n">metrics_t2i</span> <span class="o">=</span> <span class="nf">compute_retrieval_metrics</span><span class="p">(</span><span class="n">similarity</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Validation Accuracy — Text→Image: R1 </span><span class="si">{</span><span class="n">metrics_t2i</span><span class="p">[</span><span class="sh">'</span><span class="s">R1</span><span class="sh">'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">, R5 </span><span class="si">{</span><span class="n">metrics_t2i</span><span class="p">[</span><span class="sh">'</span><span class="s">R5</span><span class="sh">'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">, R10 </span><span class="si">{</span><span class="n">metrics_t2i</span><span class="p">[</span><span class="sh">'</span><span class="s">R10</span><span class="sh">'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Epoch 1 [Training]: 100%|██████████| 16/16 [00:21&lt;00:00,  1.33s/it]

Epoch 1 — Training Loss: 4.1814

Epoch 1 [Validation]: 100%|██████████| 2/2 [00:01&lt;00:00,  1.18it/s]
Validation Accuracy — Image→Text: R1 1.0000, R5 5.0000, R10 12.0000
Validation Accuracy — Text→Image: R1 1.0000, R5 5.0000, R10 11.0000

Epoch 2 [Training]: 100%|██████████| 16/16 [00:19&lt;00:00,  1.20s/it]

Epoch 2 — Training Loss: 4.0629

Epoch 2 [Validation]: 100%|██████████| 2/2 [00:01&lt;00:00,  1.25it/s]

Validation Accuracy — Image→Text: R1 3.0000, R5 6.0000, R10 15.0000
Validation Accuracy — Text→Image: R1 1.0000, R5 5.0000, R10 14.0000

Epoch 3 [Training]: 100%|██████████| 16/16 [00:19&lt;00:00,  1.22s/it]

Epoch 3 — Training Loss: 3.7348

Epoch 3 [Validation]: 100%|██████████| 2/2 [00:01&lt;00:00,  1.20it/s]

Validation Accuracy — Image→Text: R1 5.0000, R5 13.0000, R10 25.0000
Validation Accuracy — Text→Image: R1 3.0000, R5 12.0000, R10 21.0000

Epoch 4 [Training]: 100%|██████████| 16/16 [00:20&lt;00:00,  1.27s/it]

Epoch 4 — Training Loss: 3.1225

Epoch 4 [Validation]: 100%|██████████| 2/2 [00:01&lt;00:00,  1.25it/s]

Validation Accuracy — Image→Text: R1 3.0000, R5 11.0000, R10 21.0000
Validation Accuracy — Text→Image: R1 4.0000, R5 12.0000, R10 18.0000

Epoch 5 [Training]: 100%|██████████| 16/16 [00:20&lt;00:00,  1.26s/it]

Epoch 5 — Training Loss: 2.5640

Epoch 5 [Validation]: 100%|██████████| 2/2 [00:01&lt;00:00,  1.25it/s]

Validation Accuracy — Image→Text: R1 3.0000, R5 14.0000, R10 27.0000
Validation Accuracy — Text→Image: R1 1.0000, R5 13.0000, R10 26.0000

Epoch 6 [Training]: 100%|██████████| 16/16 [00:20&lt;00:00,  1.25s/it]

Epoch 6 — Training Loss: 1.9938

Epoch 6 [Validation]: 100%|██████████| 2/2 [00:01&lt;00:00,  1.04it/s]

Validation Accuracy — Image→Text: R1 5.0000, R5 12.0000, R10 17.0000
Validation Accuracy — Text→Image: R1 3.0000, R5 11.0000, R10 18.0000

Epoch 7 [Training]: 100%|██████████| 16/16 [00:20&lt;00:00,  1.29s/it]

Epoch 7 — Training Loss: 1.5335

Epoch 7 [Validation]: 100%|██████████| 2/2 [00:01&lt;00:00,  1.27it/s]

Validation Accuracy — Image→Text: R1 6.0000, R5 18.0000, R10 25.0000
Validation Accuracy — Text→Image: R1 2.0000, R5 13.0000, R10 26.0000

Epoch 8 [Training]: 100%|██████████| 16/16 [00:20&lt;00:00,  1.30s/it]

Epoch 8 — Training Loss: 1.0845

Epoch 8 [Validation]: 100%|██████████| 2/2 [00:01&lt;00:00,  1.25it/s]

Validation Accuracy — Image→Text: R1 4.0000, R5 16.0000, R10 26.0000
Validation Accuracy — Text→Image: R1 5.0000, R5 17.0000, R10 27.0000

Epoch 9 [Training]: 100%|██████████| 16/16 [00:20&lt;00:00,  1.28s/it]

Epoch 9 — Training Loss: 0.8292

Epoch 9 [Validation]: 100%|██████████| 2/2 [00:01&lt;00:00,  1.12it/s]

Validation Accuracy — Image→Text: R1 4.0000, R5 17.0000, R10 28.0000
Validation Accuracy — Text→Image: R1 1.0000, R5 16.0000, R10 24.0000

Epoch 10 [Training]: 100%|██████████| 16/16 [00:20&lt;00:00,  1.27s/it]

Epoch 10 — Training Loss: 0.6429

Epoch 10 [Validation]: 100%|██████████| 2/2 [00:01&lt;00:00,  1.25it/s]

Validation Accuracy — Image→Text: R1 6.0000, R5 17.0000, R10 28.0000
Validation Accuracy — Text→Image: R1 4.0000, R5 20.0000, R10 26.0000
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Plot all the values inside loss_values array and temperature_values
</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">loss_values</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Training Loss over Steps</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Step</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Loss</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">temperature_values</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Learned Temperature over Steps</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Step</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Temperature</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p><img src="assets/img/CLIP_blog/clip_loss.png" alt="Figure description"> <img src="assets/img/CLIP_blog/clip_temperature.png" alt="Figure description"></p> <h2 id="section-4-validation-session">Section 4: Validation session</h2> <p><a href="https://github.com/mlfoundations/open_clip" rel="external nofollow noopener" target="_blank">OpenCLIP library</a>, is an open source reimplementation of OpenAI’s CLIP (Contrastive Language-Image Pre-training) model. It provides a flexible and scalable codebase that allows researchers and developers to train, fine-tune, and deploy CLIP models on various datasets and hardware configurations.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">open_clip</span>

<span class="n">available_models</span> <span class="o">=</span> <span class="n">open_clip</span><span class="p">.</span><span class="nf">list_pretrained</span><span class="p">()</span>

<span class="k">for</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">pretrained_dataset</span> <span class="ow">in</span> <span class="n">available_models</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Model: </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s">, Pretrained on: </span><span class="si">{</span><span class="n">pretrained_dataset</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model: RN50, Pretrained on: openai
Model: RN50, Pretrained on: yfcc15m
Model: RN50, Pretrained on: cc12m
Model: RN101, Pretrained on: openai
Model: RN101, Pretrained on: yfcc15m
Model: RN50x4, Pretrained on: openai
Model: RN50x16, Pretrained on: openai
Model: RN50x64, Pretrained on: openai
Model: ViT-B-32, Pretrained on: openai
Model: ViT-B-32, Pretrained on: laion400m_e31
Model: ViT-B-32, Pretrained on: laion400m_e32
Model: ViT-B-32, Pretrained on: laion2b_e16
Model: ViT-B-32, Pretrained on: laion2b_s34b_b79k
Model: ViT-B-32, Pretrained on: datacomp_xl_s13b_b90k
Model: ViT-B-32, Pretrained on: datacomp_m_s128m_b4k
Model: ViT-B-32, Pretrained on: commonpool_m_clip_s128m_b4k
Model: ViT-B-32, Pretrained on: commonpool_m_laion_s128m_b4k
Model: ViT-B-32, Pretrained on: commonpool_m_image_s128m_b4k
Model: ViT-B-32, Pretrained on: commonpool_m_text_s128m_b4k
Model: ViT-B-32, Pretrained on: commonpool_m_basic_s128m_b4k
Model: ViT-B-32, Pretrained on: commonpool_m_s128m_b4k
Model: ViT-B-32, Pretrained on: datacomp_s_s13m_b4k
Model: ViT-B-32, Pretrained on: commonpool_s_clip_s13m_b4k
Model: ViT-B-32, Pretrained on: commonpool_s_laion_s13m_b4k
Model: ViT-B-32, Pretrained on: commonpool_s_image_s13m_b4k
Model: ViT-B-32, Pretrained on: commonpool_s_text_s13m_b4k
Model: ViT-B-32, Pretrained on: commonpool_s_basic_s13m_b4k
Model: ViT-B-32, Pretrained on: commonpool_s_s13m_b4k
Model: ViT-B-32, Pretrained on: metaclip_400m
Model: ViT-B-32, Pretrained on: metaclip_fullcc
Model: ViT-B-32-256, Pretrained on: datacomp_s34b_b86k
Model: ViT-B-16, Pretrained on: openai
Model: ViT-B-16, Pretrained on: laion400m_e31
Model: ViT-B-16, Pretrained on: laion400m_e32
Model: ViT-B-16, Pretrained on: laion2b_s34b_b88k
Model: ViT-B-16, Pretrained on: datacomp_xl_s13b_b90k
Model: ViT-B-16, Pretrained on: datacomp_l_s1b_b8k
Model: ViT-B-16, Pretrained on: commonpool_l_clip_s1b_b8k
Model: ViT-B-16, Pretrained on: commonpool_l_laion_s1b_b8k
Model: ViT-B-16, Pretrained on: commonpool_l_image_s1b_b8k
Model: ViT-B-16, Pretrained on: commonpool_l_text_s1b_b8k
Model: ViT-B-16, Pretrained on: commonpool_l_basic_s1b_b8k
Model: ViT-B-16, Pretrained on: commonpool_l_s1b_b8k
Model: ViT-B-16, Pretrained on: dfn2b
Model: ViT-B-16, Pretrained on: metaclip_400m
Model: ViT-B-16, Pretrained on: metaclip_fullcc
Model: ViT-B-16-plus-240, Pretrained on: laion400m_e31
Model: ViT-B-16-plus-240, Pretrained on: laion400m_e32
Model: ViT-L-14, Pretrained on: openai
Model: ViT-L-14, Pretrained on: laion400m_e31
Model: ViT-L-14, Pretrained on: laion400m_e32
Model: ViT-L-14, Pretrained on: laion2b_s32b_b82k
Model: ViT-L-14, Pretrained on: datacomp_xl_s13b_b90k
Model: ViT-L-14, Pretrained on: commonpool_xl_clip_s13b_b90k
Model: ViT-L-14, Pretrained on: commonpool_xl_laion_s13b_b90k
Model: ViT-L-14, Pretrained on: commonpool_xl_s13b_b90k
Model: ViT-L-14, Pretrained on: metaclip_400m
Model: ViT-L-14, Pretrained on: metaclip_fullcc
Model: ViT-L-14, Pretrained on: dfn2b
Model: ViT-L-14, Pretrained on: dfn2b_s39b
Model: ViT-L-14-336, Pretrained on: openai
Model: ViT-H-14, Pretrained on: laion2b_s32b_b79k
Model: ViT-H-14, Pretrained on: metaclip_fullcc
Model: ViT-H-14, Pretrained on: metaclip_altogether
Model: ViT-H-14, Pretrained on: dfn5b
Model: ViT-H-14-378, Pretrained on: dfn5b
Model: ViT-g-14, Pretrained on: laion2b_s12b_b42k
Model: ViT-g-14, Pretrained on: laion2b_s34b_b88k
Model: ViT-bigG-14, Pretrained on: laion2b_s39b_b160k
Model: ViT-bigG-14, Pretrained on: metaclip_fullcc
Model: roberta-ViT-B-32, Pretrained on: laion2b_s12b_b32k
Model: xlm-roberta-base-ViT-B-32, Pretrained on: laion5b_s13b_b90k
Model: xlm-roberta-large-ViT-H-14, Pretrained on: frozen_laion5b_s13b_b90k
Model: convnext_base, Pretrained on: laion400m_s13b_b51k
Model: convnext_base_w, Pretrained on: laion2b_s13b_b82k
Model: convnext_base_w, Pretrained on: laion2b_s13b_b82k_augreg
Model: convnext_base_w, Pretrained on: laion_aesthetic_s13b_b82k
Model: convnext_base_w_320, Pretrained on: laion_aesthetic_s13b_b82k
Model: convnext_base_w_320, Pretrained on: laion_aesthetic_s13b_b82k_augreg
Model: convnext_large_d, Pretrained on: laion2b_s26b_b102k_augreg
Model: convnext_large_d_320, Pretrained on: laion2b_s29b_b131k_ft
Model: convnext_large_d_320, Pretrained on: laion2b_s29b_b131k_ft_soup
Model: convnext_xxlarge, Pretrained on: laion2b_s34b_b82k_augreg
Model: convnext_xxlarge, Pretrained on: laion2b_s34b_b82k_augreg_rewind
Model: convnext_xxlarge, Pretrained on: laion2b_s34b_b82k_augreg_soup
Model: coca_ViT-B-32, Pretrained on: laion2b_s13b_b90k
Model: coca_ViT-B-32, Pretrained on: mscoco_finetuned_laion2b_s13b_b90k
Model: coca_ViT-L-14, Pretrained on: laion2b_s13b_b90k
Model: coca_ViT-L-14, Pretrained on: mscoco_finetuned_laion2b_s13b_b90k
Model: EVA01-g-14, Pretrained on: laion400m_s11b_b41k
Model: EVA01-g-14-plus, Pretrained on: merged2b_s11b_b114k
Model: EVA02-B-16, Pretrained on: merged2b_s8b_b131k
Model: EVA02-L-14, Pretrained on: merged2b_s4b_b131k
Model: EVA02-L-14-336, Pretrained on: merged2b_s6b_b61k
Model: EVA02-E-14, Pretrained on: laion2b_s4b_b115k
Model: EVA02-E-14-plus, Pretrained on: laion2b_s9b_b144k
Model: ViT-B-16-SigLIP, Pretrained on: webli
Model: ViT-B-16-SigLIP-256, Pretrained on: webli
Model: ViT-B-16-SigLIP-i18n-256, Pretrained on: webli
Model: ViT-B-16-SigLIP-384, Pretrained on: webli
Model: ViT-B-16-SigLIP-512, Pretrained on: webli
Model: ViT-L-16-SigLIP-256, Pretrained on: webli
Model: ViT-L-16-SigLIP-384, Pretrained on: webli
Model: ViT-SO400M-14-SigLIP, Pretrained on: webli
Model: ViT-SO400M-16-SigLIP-i18n-256, Pretrained on: webli
Model: ViT-SO400M-14-SigLIP-378, Pretrained on: webli
Model: ViT-SO400M-14-SigLIP-384, Pretrained on: webli
Model: ViT-B-32-SigLIP2-256, Pretrained on: webli
Model: ViT-B-16-SigLIP2, Pretrained on: webli
Model: ViT-B-16-SigLIP2-256, Pretrained on: webli
Model: ViT-B-16-SigLIP2-384, Pretrained on: webli
Model: ViT-B-16-SigLIP2-512, Pretrained on: webli
Model: ViT-L-16-SigLIP2-256, Pretrained on: webli
Model: ViT-L-16-SigLIP2-384, Pretrained on: webli
Model: ViT-L-16-SigLIP2-512, Pretrained on: webli
Model: ViT-SO400M-14-SigLIP2, Pretrained on: webli
Model: ViT-SO400M-14-SigLIP2-378, Pretrained on: webli
Model: ViT-SO400M-16-SigLIP2-256, Pretrained on: webli
Model: ViT-SO400M-16-SigLIP2-384, Pretrained on: webli
Model: ViT-SO400M-16-SigLIP2-512, Pretrained on: webli
Model: ViT-gopt-16-SigLIP2-256, Pretrained on: webli
Model: ViT-gopt-16-SigLIP2-384, Pretrained on: webli
Model: ViT-L-14-CLIPA, Pretrained on: datacomp1b
Model: ViT-L-14-CLIPA-336, Pretrained on: datacomp1b
Model: ViT-H-14-CLIPA, Pretrained on: datacomp1b
Model: ViT-H-14-CLIPA-336, Pretrained on: laion2b
Model: ViT-H-14-CLIPA-336, Pretrained on: datacomp1b
Model: ViT-bigG-14-CLIPA, Pretrained on: datacomp1b
Model: ViT-bigG-14-CLIPA-336, Pretrained on: datacomp1b
Model: nllb-clip-base, Pretrained on: v1
Model: nllb-clip-large, Pretrained on: v1
Model: nllb-clip-base-siglip, Pretrained on: v1
Model: nllb-clip-base-siglip, Pretrained on: mrl
Model: nllb-clip-large-siglip, Pretrained on: v1
Model: nllb-clip-large-siglip, Pretrained on: mrl
Model: MobileCLIP-S1, Pretrained on: datacompdr
Model: MobileCLIP-S2, Pretrained on: datacompdr
Model: MobileCLIP-B, Pretrained on: datacompdr
Model: MobileCLIP-B, Pretrained on: datacompdr_lt
Model: ViTamin-S, Pretrained on: datacomp1b
Model: ViTamin-S-LTT, Pretrained on: datacomp1b
Model: ViTamin-B, Pretrained on: datacomp1b
Model: ViTamin-B-LTT, Pretrained on: datacomp1b
Model: ViTamin-L, Pretrained on: datacomp1b
Model: ViTamin-L-256, Pretrained on: datacomp1b
Model: ViTamin-L-336, Pretrained on: datacomp1b
Model: ViTamin-L-384, Pretrained on: datacomp1b
Model: ViTamin-L2, Pretrained on: datacomp1b
Model: ViTamin-L2-256, Pretrained on: datacomp1b
Model: ViTamin-L2-336, Pretrained on: datacomp1b
Model: ViTamin-L2-384, Pretrained on: datacomp1b
Model: ViTamin-XL-256, Pretrained on: datacomp1b
Model: ViTamin-XL-336, Pretrained on: datacomp1b
Model: ViTamin-XL-384, Pretrained on: datacomp1b
Model: RN50-quickgelu, Pretrained on: openai
Model: RN50-quickgelu, Pretrained on: yfcc15m
Model: RN50-quickgelu, Pretrained on: cc12m
Model: RN101-quickgelu, Pretrained on: openai
Model: RN101-quickgelu, Pretrained on: yfcc15m
Model: RN50x4-quickgelu, Pretrained on: openai
Model: RN50x16-quickgelu, Pretrained on: openai
Model: RN50x64-quickgelu, Pretrained on: openai
Model: ViT-B-32-quickgelu, Pretrained on: openai
Model: ViT-B-32-quickgelu, Pretrained on: laion400m_e31
Model: ViT-B-32-quickgelu, Pretrained on: laion400m_e32
Model: ViT-B-32-quickgelu, Pretrained on: metaclip_400m
Model: ViT-B-32-quickgelu, Pretrained on: metaclip_fullcc
Model: ViT-B-16-quickgelu, Pretrained on: openai
Model: ViT-B-16-quickgelu, Pretrained on: dfn2b
Model: ViT-B-16-quickgelu, Pretrained on: metaclip_400m
Model: ViT-B-16-quickgelu, Pretrained on: metaclip_fullcc
Model: ViT-L-14-quickgelu, Pretrained on: openai
Model: ViT-L-14-quickgelu, Pretrained on: metaclip_400m
Model: ViT-L-14-quickgelu, Pretrained on: metaclip_fullcc
Model: ViT-L-14-quickgelu, Pretrained on: dfn2b
Model: ViT-L-14-336-quickgelu, Pretrained on: openai
Model: ViT-H-14-quickgelu, Pretrained on: metaclip_fullcc
Model: ViT-H-14-quickgelu, Pretrained on: dfn5b
Model: ViT-H-14-378-quickgelu, Pretrained on: dfn5b
Model: ViT-bigG-14-quickgelu, Pretrained on: metaclip_fullcc
</code></pre></div></div> <p>Check out here:</p> <p><a href="https://github.com/mlfoundations/open_clip/blob/main/docs/openclip_retrieval_results.csv" rel="external nofollow noopener" target="_blank">https://github.com/mlfoundations/open_clip/blob/main/docs/openclip_retrieval_results.csv</a></p> <p><a href="https://github.com/mlfoundations/open_clip/blob/main/docs/openclip_classification_results.csv" rel="external nofollow noopener" target="_blank">https://github.com/mlfoundations/open_clip/blob/main/docs/openclip_classification_results.csv</a></p> <p>Load a pretrained model from OpenCLIP:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">model_name</span> <span class="o">=</span> <span class="sh">'</span><span class="s">ViT-SO400M-14-SigLIP-384</span><span class="sh">'</span>
<span class="n">model_pretrained</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">preprocess</span> <span class="o">=</span> <span class="n">open_clip</span><span class="p">.</span><span class="nf">create_model_and_transforms</span><span class="p">(</span>
    <span class="n">model_name</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="sh">'</span><span class="s">webli</span><span class="sh">'</span>
<span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">open_clip</span><span class="p">.</span><span class="nf">get_tokenizer</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">model_pretrained</span> <span class="o">=</span> <span class="n">model_pretrained</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">).</span><span class="nf">eval</span><span class="p">()</span>
</code></pre></div></div> <p>Evaluate the performance of pretrained model on the vanilla provided test dataset:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># --- DATASET ---
</span>
<span class="n">val_dataset</span> <span class="o">=</span>  <span class="nc">CocoCaptions</span><span class="p">(</span>
    <span class="n">root</span><span class="o">=</span><span class="sh">'</span><span class="s">./mini_coco/val2017</span><span class="sh">'</span><span class="p">,</span>
    <span class="n">annFile</span><span class="o">=</span><span class="sh">'</span><span class="s">./mini_coco/annotations/captions_val2017.json</span><span class="sh">'</span><span class="p">,</span>
    <span class="n">transform</span><span class="o">=</span><span class="n">preprocess</span><span class="p">)</span>


<span class="n">val_loader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                        <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_fn</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># --- VALIDATION ---
</span><span class="n">model_pretrained</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="n">all_image_features</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">all_text_features</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">texts</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">val_loader</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">[Validation]</span><span class="sh">"</span><span class="p">):</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
        <span class="n">texts</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="n">texts</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>

        <span class="n">image_feats</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">normalize</span><span class="p">(</span><span class="n">model_pretrained</span><span class="p">.</span><span class="nf">encode_image</span><span class="p">(</span><span class="n">images</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">text_feats</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">normalize</span><span class="p">(</span><span class="n">model_pretrained</span><span class="p">.</span><span class="nf">encode_text</span><span class="p">(</span><span class="n">texts</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">all_image_features</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">image_feats</span><span class="p">)</span>
        <span class="n">all_text_features</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">text_feats</span><span class="p">)</span>

    <span class="n">image_features</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">(</span><span class="n">all_image_features</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">text_features</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">(</span><span class="n">all_text_features</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">similarity</span> <span class="o">=</span> <span class="n">image_features</span> <span class="o">@</span> <span class="n">text_features</span><span class="p">.</span><span class="n">T</span>

    <span class="n">metrics_i2t</span> <span class="o">=</span> <span class="nf">compute_retrieval_metrics</span><span class="p">(</span><span class="n">similarity</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Validation Accuracy — Image→Text: R1 </span><span class="si">{</span><span class="n">metrics_i2t</span><span class="p">[</span><span class="sh">'</span><span class="s">R1</span><span class="sh">'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">, R5 </span><span class="si">{</span><span class="n">metrics_i2t</span><span class="p">[</span><span class="sh">'</span><span class="s">R5</span><span class="sh">'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">, R10 </span><span class="si">{</span><span class="n">metrics_i2t</span><span class="p">[</span><span class="sh">'</span><span class="s">R10</span><span class="sh">'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">metrics_t2i</span> <span class="o">=</span> <span class="nf">compute_retrieval_metrics</span><span class="p">(</span><span class="n">similarity</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Validation Accuracy — Text→Image: R1 </span><span class="si">{</span><span class="n">metrics_t2i</span><span class="p">[</span><span class="sh">'</span><span class="s">R1</span><span class="sh">'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">, R5 </span><span class="si">{</span><span class="n">metrics_t2i</span><span class="p">[</span><span class="sh">'</span><span class="s">R5</span><span class="sh">'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">, R10 </span><span class="si">{</span><span class="n">metrics_t2i</span><span class="p">[</span><span class="sh">'</span><span class="s">R10</span><span class="sh">'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[Validation]: 100%|██████████| 2/2 [00:15&lt;00:00,  7.70s/it]

Validation Accuracy — Image→Text: R1 92.00, R5 100.00, R10 100.00
Validation Accuracy — Text→Image: R1 88.00, R5 99.00, R10 100.00
</code></pre></div></div> <p>Feel free to test different pretrained models.</p> <h2 id="section-5-zero-shot-image-classification">Section 5: Zero-Shot Image Classification</h2> <p>In this section we will test the ability of CLIP pretrained encoders in a typical downstream task: <strong>Zero-Shot Image Classification</strong>.</p> <p>CIFAR10 test dataset could be used for this purpose. It contains 10k test images with 10 testing labels: "airplane", "automobile", "bird", "cat", "deer", "dog", "frog", "horse", "ship" and "truck"</p> <p>Load the CLIP pretrained encoders (we know how to do it).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Load the model and preprocessing
</span><span class="n">model_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">ViT-B-32</span><span class="sh">"</span>
<span class="n">pretrained</span> <span class="o">=</span> <span class="sh">"</span><span class="s">openai</span><span class="sh">"</span>
<span class="n">device</span> <span class="o">=</span> <span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span>

<span class="n">model_pretrained</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">preprocess</span> <span class="o">=</span> <span class="n">open_clip</span><span class="p">.</span><span class="nf">create_model_and_transforms</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="n">pretrained</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">open_clip</span><span class="p">.</span><span class="nf">get_tokenizer</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">model_pretrained</span> <span class="o">=</span> <span class="n">model_pretrained</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">).</span><span class="nf">eval</span><span class="p">()</span>
</code></pre></div></div> <p>Load the test dataset.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CIFAR10</span>
<span class="c1"># Load CIFAR-10 test set
</span><span class="n">cifar10_labels</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">"</span><span class="s">airplane</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">automobile</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">bird</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">cat</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">deer</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">dog</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">frog</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">horse</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">ship</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">truck</span><span class="sh">"</span>
<span class="p">]</span>
<span class="n">transform</span> <span class="o">=</span> <span class="n">preprocess</span>  <span class="c1"># Use OpenCLIP's default preprocess
</span><span class="n">testset</span> <span class="o">=</span> <span class="nc">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="sh">"</span><span class="s">./data</span><span class="sh">"</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="n">testloader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">testset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div> <p>Extract Text Embeddings, one for each label.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Tokenize class names and extract embeddings
</span><span class="n">text_inputs</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">([</span><span class="sa">f</span><span class="sh">"</span><span class="s">a photo of a </span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="sh">"</span> <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">cifar10_labels</span><span class="p">]).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="n">text_features</span> <span class="o">=</span> <span class="n">model_pretrained</span><span class="p">.</span><span class="nf">encode_text</span><span class="p">(</span><span class="n">text_inputs</span><span class="p">)</span>
    <span class="n">text_features</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">normalize</span><span class="p">(</span><span class="n">text_features</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <p>Extract all the image embeddings and for each image embedding compute the cosine similarity score between itself and all the text embeddings. The predicted label for a given input image will be the item with the highest similarity score.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Classify each sample in the test set
</span><span class="n">all_preds</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">all_targets</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">targets</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">testloader</span><span class="p">):</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">image_features</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">encode_image</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">image_features</span> <span class="o">/=</span> <span class="n">image_features</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="n">logits</span> <span class="o">=</span> <span class="n">image_features</span> <span class="o">@</span> <span class="n">text_features</span><span class="p">.</span><span class="n">T</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">all_preds</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="n">preds</span><span class="p">.</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">())</span>
        <span class="n">all_targets</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="n">targets</span><span class="p">.</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">())</span>
</code></pre></div></div> <p>Compute classification metrics</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span><span class="p">,</span> <span class="n">accuracy_score</span>

<span class="n">acc</span> <span class="o">=</span> <span class="nf">accuracy_score</span><span class="p">(</span><span class="n">all_targets</span><span class="p">,</span> <span class="n">all_preds</span><span class="p">)</span>
<span class="n">report</span> <span class="o">=</span> <span class="nf">classification_report</span><span class="p">(</span><span class="n">all_targets</span><span class="p">,</span> <span class="n">all_preds</span><span class="p">,</span> <span class="n">target_names</span><span class="o">=</span><span class="n">cifar10_labels</span><span class="p">,</span> <span class="n">digits</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s"> Zero-Shot Image Classification Accuracy on CIFAR-10: </span><span class="si">{</span><span class="n">acc</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s"> Detailed Classification Report:</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">report</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> Zero-Shot Image Classification Accuracy on CIFAR-10: 86.17%

 Detailed Classification Report:

              precision    recall  f1-score   support

    airplane     0.9807    0.8110    0.8878      1000
  automobile     0.9768    0.8420    0.9044      1000
        bird     0.7494    0.8730    0.8065      1000
         cat     0.8565    0.7340    0.7905      1000
        deer     0.8070    0.8570    0.8312      1000
         dog     0.8138    0.8740    0.8428      1000
        frog     0.9529    0.7080    0.8124      1000
       horse     0.8140    0.9670    0.8839      1000
        ship     0.9179    0.9730    0.9447      1000
       truck     0.8417    0.9780    0.9047      1000

    accuracy                         0.8617     10000
   macro avg     0.8710    0.8617    0.8609     10000
weighted avg     0.8710    0.8617    0.8609     10000
</code></pre></div></div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Giordano Cicchetti . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script defer src="https://cdn.jsdelivr.net/npm/lightbox2@2.11.5/dist/js/lightbox.min.js" integrity="sha256-A6jI5V9s1JznkWwsBaRK8kSeXLgIqQfxfnvdDOZEURY=" crossorigin="anonymous"></script> <script type="module">
      import PhotoSwipeLightbox from 'https://cdn.jsdelivr.net/npm/photoswipe@5.4.4/dist/photoswipe-lightbox.esm.min.js';
      import PhotoSwipe from 'https://cdn.jsdelivr.net/npm/photoswipe@5.4.4/dist/photoswipe.esm.min.js';
      const photoswipe = new PhotoSwipeLightbox({
        gallery: '.pswp-gallery',
        children: 'a',
        pswpModule: PhotoSwipe,
      });
      photoswipe.init();
    </script> <script defer src="https://cdn.jsdelivr.net/npm/spotlight.js@0.7.8/dist/spotlight.bundle.min.js" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/venobox@2.1.8/dist/venobox.min.js" integrity="sha256-LsGXHsHMMmTcz3KqTaWvLv6ome+7pRiic2LPnzTfiSo=" crossorigin="anonymous"></script> <script>document.addEventListener("readystatechange",()=>{"complete"===document.readyState&&new VenoBox});</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>